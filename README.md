# –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• - –®–ü–ê–†–ì–ê–õ–ö–ê –ö –≠–ö–ó–ê–ú–ï–ù–£
**–î–∞—Ç–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏:** 21 —è–Ω–≤–∞—Ä—è 2026 | **–≠–∫–∑–∞–º–µ–Ω:** 23 —è–Ω–≤–∞—Ä—è 2026

---

## –¢–ï–ú–ê 1: –¢–ò–ü–´ –î–ê–ù–ù–´–• –ò –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø

### üîë –ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è

**–°–ª—É—á–∞–π–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞** - –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è, –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
- **–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è** (—Å–ª—É—á–∞–π–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π)
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è** (–∑–Ω–∞—á–µ–Ω–∏–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ)

**–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞** - –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ

### –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

| –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | –§–æ—Ä–º—É–ª–∞ | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ | –ö–æ–¥ |
|---|---|---|---|
| **–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ (–ì–∞—É—Å—Å–∞)** | $\mu, \sigma$ | –ü—Ä–∏—Ä–æ–¥–Ω—ã–µ —è–≤–ª–µ–Ω–∏—è, —Ç–µ—Å—Ç–æ–≤—ã–µ –±–∞–ª–ª—ã | `np.random.normal(mean, std, size)` |
| **–ü—É–∞—Å—Å–æ–Ω–∞** | $\lambda$ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –≤ –≤—Ä–µ–º–µ–Ω–∏ | `np.random.poisson(lam, size)` |
| **–ë–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ** | $n, p$ | –£—Å–ø–µ—Ö/–Ω–µ—É–¥–∞—á–∞ –∏—Å–ø—ã—Ç–∞–Ω–∏–π | `np.random.binomial(n, p, size)` |
| **–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ** | $\lambda$ | –í—Ä–µ–º—è –¥–æ —Å–æ–±—ã—Ç–∏—è | `np.random.exponential(scale, size)` |
| **–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ** | $a, b$ | –°–ª—É—á–∞–π–Ω–æ–µ —á–∏—Å–ª–æ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ | `np.random.uniform(a, b, size)` |

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

```python
import numpy as np
from scipy import stats

data = np.random.normal(100, 15, 1000)

# –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
print(f"–°—Ä–µ–¥–Ω–µ–µ (mean): {np.mean(data)}")
print(f"–ú–µ–¥–∏–∞–Ω–∞ (median): {np.median(data)}")
print(f"–ú–æ–¥–∞ (mode): {stats.mode(data)}")
print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(data)}")
print(f"–î–∏—Å–ø–µ—Ä—Å–∏—è (variance): {np.var(data)}")
print(f"–ê—Å–∏–º–º–µ—Ç—Ä–∏—è (skewness): {stats.skew(data)}")  # 0 = —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ
print(f"–≠–∫—Å—Ü–µ—Å—Å (kurtosis): {stats.kurtosis(data)}")  # –º–µ—Ä–∞ –æ—Å—Ç—Ä–æ—Ç—ã –ø–∏–∫–∞

# –ö–≤–∞—Ä—Ç–∏–ª–∏
print(f"Q1: {np.percentile(data, 25)}, Q3: {np.percentile(data, 75)}")
```

### –°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–∏

**–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–∞—è —Å–µ—Ä–∏—è:**
- –°—Ä–µ–¥–Ω–µ–µ const (–Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
- –î–∏—Å–ø–µ—Ä—Å–∏—è const
- –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç –ª–∞–≥–∞, –Ω–µ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏

**–ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–∞—è —Å–µ—Ä–∏—è:**
- –¢—Ä–µ–Ω–¥ (—Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Ç–µ—Ç/–ø–∞–¥–∞–µ—Ç)
- –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
- –ò–∑–º–µ–Ω—è—é—â–∞—è—Å—è –¥–∏—Å–ø–µ—Ä—Å–∏—è

### –ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–π –≤ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—É—é

```python
from statsmodels.tsa.stattools import adfuller
import pandas as pd

# –¢–µ—Å—Ç –Ω–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å (ADF —Ç–µ—Å—Ç)
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    print(f'ADF Statistic: {result[0]:.6f}')
    print(f'p-value: {result[1]:.6f}')
    print(f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: {result[4]}')
    
    if result[1] <= 0.05:
        print("‚úì –°–µ—Ä–∏—è —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–∞ (p < 0.05)")
    else:
        print("‚úó –°–µ—Ä–∏—è –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–∞ (p > 0.05)")
    return result[1] <= 0.05

# –ú–ï–¢–û–î–´ –ü–†–ï–í–†–ê–©–ï–ù–ò–Ø:

# 1. Differencing (—Ä–∞–∑–Ω–æ—Å—Ç–∏)
ts_diff = data.diff().dropna()

# 2. Log-transform
ts_log = np.log(data)

# 3. –î–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–¥
from scipy.signal import detrend
ts_detrended = detrend(data)

# 4. Seasonal decomposition
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(data, model='additive', period=12)
ts_deseasonalized = data - decomposition.seasonal
```

---

## –¢–ï–ú–ê 2: –°–í–Ø–ó–ò –ú–ï–ñ–î–£ –ü–ï–†–ï–ú–ï–ù–ù–´–ú–ò

### üìä –ß–∏—Å–ª–æ–≤—ã–µ ‚Üî –ß–∏—Å–ª–æ–≤—ã–µ

```python
import numpy as np
from scipy.stats import pearsonr, spearmanr, kendalltau
import matplotlib.pyplot as plt

x = np.random.randn(100)
y = 2*x + np.random.randn(100)

# –ö–û–†–†–ï–õ–Ø–¶–ò–Ø –ü–ò–†–°–û–ù–ê (–ª–∏–Ω–µ–π–Ω–∞—è —Å–≤—è–∑—å, –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥)
corr_pearson, p_value = pearsonr(x, y)
print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞: {corr_pearson:.3f}, p={p_value:.4f}")

# –ö–û–†–†–ï–õ–Ø–¶–ò–Ø –°–ü–ò–†–ú–ï–ù–ê (—Ä–∞–Ω–≥–æ–≤–∞—è, –º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è —Å–≤—è–∑—å)
corr_spearman, p_value = spearmanr(x, y)
print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –°–ø–∏—Ä–º–µ–Ω–∞: {corr_spearman:.3f}, p={p_value:.4f}")

# –ö–û–†–†–ï–õ–Ø–¶–ò–Ø –ö–ï–ù–î–ê–õ–õ–ê (–¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫)
corr_kendall, p_value = kendalltau(x, y)
print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ö–µ–Ω–¥–∞–ª–ª–∞: {corr_kendall:.3f}, p={p_value:.4f}")

# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
import pandas as pd
df = pd.DataFrame({'X': x, 'Y': y})
corr_matrix = df.corr(method='pearson')  # –∏–ª–∏ 'spearman'
print(corr_matrix)
```

### üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ‚Üî –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ

```python
from scipy.stats import chi2_contingency
import numpy as np
import pandas as pd

# –¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏ (contingency table)
data = pd.DataFrame({
    'Gender': ['M', 'M', 'F', 'F', 'M', 'F'],
    'Product': ['A', 'B', 'A', 'B', 'A', 'B']
})

contingency_table = pd.crosstab(data['Gender'], data['Product'])
print(contingency_table)
#          A  B
# Gender      
# F        2  1
# M        2  1

# CHI-SQUARE —Ç–µ—Å—Ç
chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square: {chi2:.4f}, p-value: {p_value:.4f}")

# CRAM√âR'S V (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ä–∞ —Å–≤—è–∑–∏ 0-1)
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2, p, dof, ex = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    min_dim = min(confusion_matrix.shape) - 1
    return np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0

v = cramers_v(data['Gender'], data['Product'])
print(f"Cram√©r's V: {v:.3f}")  # 0 = –Ω–µ—Ç —Å–≤—è–∑–∏, 1 = –ø–æ–ª–Ω–∞—è —Å–≤—è–∑—å
```

### üìä –ß–∏—Å–ª–æ–≤–∞—è ‚Üî –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è

```python
from scipy.stats import f_oneway, pointbiserialr
import pandas as pd

# Point-biserial (—Ç–æ–ª—å–∫–æ 2 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
# –ß–∏—Å–ª–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è vs –±–∏–Ω–∞—Ä–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è
x = np.random.randn(100)
category = np.random.randint(0, 2, 100)  # 0 –∏–ª–∏ 1

corr_pb, p_value = pointbiserialr(category, x)
print(f"Point-biserial –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {corr_pb:.3f}, p={p_value:.4f}")

# ANOVA (–Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä—É–ø–ø)
df = pd.DataFrame({
    'value': np.random.randn(300),
    'group': np.repeat(['A', 'B', 'C'], 100)
})

groupA = df[df['group'] == 'A']['value']
groupB = df[df['group'] == 'B']['value']
groupC = df[df['group'] == 'C']['value']

f_stat, p_value = f_oneway(groupA, groupB, groupC)
print(f"F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {f_stat:.4f}, p-value: {p_value:.4f}")
if p_value < 0.05:
    print("‚úì –ó–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏")
else:
    print("‚úó –†–∞–∑–ª–∏—á–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã")
```

---

## –¢–ï–ú–ê 3: –î–û–í–ï–†–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–¢–ï–†–í–ê–õ–´ –ò P-VALUE

### –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (CI)

```python
import numpy as np
from scipy import stats

data = np.random.normal(100, 15, 100)

# –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ (95%)
n = len(data)
mean = np.mean(data)
se = stats.sem(data)  # Standard Error

# –ú–µ—Ç–æ–¥ 1: t-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
t_critical = stats.t.ppf(0.975, n-1)  # 0.975 –¥–ª—è –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Ç–µ—Å—Ç–∞
ci_lower = mean - t_critical * se
ci_upper = mean + t_critical * se
print(f"CI 95% (t-dist): [{ci_lower:.2f}, {ci_upper:.2f}]")

# –ú–µ—Ç–æ–¥ 2: z-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫)
z_critical = stats.norm.ppf(0.975)
ci_lower_z = mean - z_critical * se
ci_upper_z = mean + z_critical * se
print(f"CI 95% (z-dist): [{ci_lower_z:.2f}, {ci_upper_z:.2f}]")

# –î–ª—è –¥–æ–ª–∏ (proportion)
successes = 56
n_total = 80
p_hat = successes / n_total
se_p = np.sqrt(p_hat * (1 - p_hat) / n_total)
ci_p_lower = p_hat - 1.96 * se_p
ci_p_upper = p_hat + 1.96 * se_p
print(f"CI –¥–ª—è –¥–æ–ª–∏ 95%: [{ci_p_lower:.3f}, {ci_p_upper:.3f}]")
```

### P-value

```python
from scipy.stats import ttest_1samp, norm

# P-value - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∏—Ç—å —Ç–∞–∫–∏–µ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã,
# –µ—Å–ª–∏ –Ω—É–ª–µ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –≤–µ—Ä–Ω–∞

# –ü—Ä–∏–º–µ—Ä: –æ–¥–Ω–æ–≤—ã–±–æ—Ä–æ—á–Ω—ã–π t-—Ç–µ—Å—Ç
data = np.random.normal(102, 15, 100)
null_mean = 100

t_stat, p_value = ttest_1samp(data, null_mean)
print(f"t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {t_stat:.4f}, p-value: {p_value:.4f}")

# –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
if p_value < 0.001:
    print("‚úì‚úì‚úì –û—á–µ–Ω—å —Å–∏–ª—å–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ü–†–û–¢–ò–í H0 (p < 0.001)")
elif p_value < 0.01:
    print("‚úì‚úì –°–∏–ª—å–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ü–†–û–¢–ò–í H0 (p < 0.01)")
elif p_value < 0.05:
    print("‚úì –£–º–µ—Ä–µ–Ω–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ü–†–û–¢–ò–í H0 (p < 0.05)")
else:
    print("‚úó –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ü–†–û–¢–ò–í H0 (p ‚â• 0.05)")

# –°–≤—è–∑—å –º–µ–∂–¥—É CI –∏ p-value
# –ï—Å–ª–∏ CI –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ H0 ‚Üí p < 0.05
```

---

## –¢–ï–ú–ê 4: –ü–†–û–í–ï–†–ö–ê –ì–ò–ü–û–¢–ï–ó (–ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã)

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ

```python
from scipy.stats import ttest_1samp, ttest_ind
import numpy as np

# –û–î–ù–û–í–´–ë–û–†–û–ß–ù–´–ô T-–¢–ï–°–¢
# H0: Œº = 100
# Ha: Œº ‚â† 100

data = np.random.normal(102, 15, 50)
null_mean = 100

t_stat, p_value = ttest_1samp(data, null_mean)
print(f"–û–¥–Ω–æ–≤—ã–±–æ—Ä–æ—á–Ω—ã–π t-—Ç–µ—Å—Ç:")
print(f"t = {t_stat:.4f}, p-value = {p_value:.4f}")

if p_value < 0.05:
    print("‚úì –û—Ç–∫–ª–æ–Ω—è–µ–º H0 - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–∏–º–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç 100")
else:
    print("‚úó –ù–µ –º–æ–∂–µ–º –æ—Ç–∫–ª–æ–Ω–∏—Ç—å H0")

# –î–í–£–•–í–´–ë–û–†–û–ß–ù–´–ô T-–¢–ï–°–¢ (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –≤—ã–±–æ—Ä–∫–∏)
sample1 = np.random.normal(100, 15, 50)
sample2 = np.random.normal(102, 15, 50)

t_stat, p_value = ttest_ind(sample1, sample2)
print(f"\n–î–≤—É—Ö–≤—ã–±–æ—Ä–æ—á–Ω—ã–π t-—Ç–µ—Å—Ç:")
print(f"t = {t_stat:.4f}, p-value = {p_value:.4f}")

if p_value < 0.05:
    print("‚úì –ì—Ä—É–ø–ø—ã –∑–Ω–∞—á–∏–º–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è")
else:
    print("‚úó –†–∞–∑–ª–∏—á–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã")
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ª–∏

```python
from scipy.stats import binom_test, binomtest

# H0: p = 0.5
# Ha: p ‚â† 0.5

successes = 60
trials = 100
null_proportion = 0.5

# –ë–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç
result = binomtest(successes, trials, null_proportion, alternative='two-sided')
print(f"–ë–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –¥–æ–ª–∏:")
print(f"p-value = {result.pvalue:.4f}")

if result.pvalue < 0.05:
    print("‚úì –î–æ–ª—è –∑–Ω–∞—á–∏–º–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç 0.5")
else:
    print("‚úó –†–∞–∑–ª–∏—á–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã")
```

---

## –¢–ï–ú–ê 5: –ü–†–û–í–ï–†–ö–ê –ì–ò–ü–û–¢–ï–ó (–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã)

### –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã

```python
from scipy.stats import mannwhitneyu, wilcoxon, kruskal, ranksums
import numpy as np

# MANN-WHITNEY U –¢–ï–°–¢ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ t-—Ç–µ—Å—Ç—É –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤—ã–±–æ—Ä–æ–∫)
# –ü—Ä–æ–≤–µ—Ä—è–µ—Ç: –º–µ–¥–∏–∞–Ω—ã –¥–≤—É—Ö –≥—Ä—É–ø–ø –æ—Ç–ª–∏—á–∞—é—Ç—Å—è?

sample1 = np.random.exponential(2, 50)  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–ª—å–∑—è –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—Ç—å
sample2 = np.random.exponential(2.5, 50)

u_stat, p_value = mannwhitneyu(sample1, sample2, alternative='two-sided')
print(f"Mann-Whitney U —Ç–µ—Å—Ç:")
print(f"U = {u_stat:.4f}, p-value = {p_value:.4f}")

if p_value < 0.05:
    print("‚úì –ú–µ–¥–∏–∞–Ω—ã –≥—Ä—É–ø–ø—ã –∑–Ω–∞—á–∏–º–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è")

# WILCOXON SIGNED-RANK –¢–ï–°–¢ (—Å–≤—è–∑–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏)
before = np.random.normal(100, 15, 30)
after = before + np.random.normal(2, 5, 30)  # –ù–µ–±–æ–ª—å—à–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ

w_stat, p_value = wilcoxon(before, after)
print(f"\nWilcoxon —Ç–µ—Å—Ç (—Å–≤—è–∑–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏):")
print(f"W = {w_stat:.4f}, p-value = {p_value:.4f}")

# KRUSKAL-WALLIS –¢–ï–°–¢ (–Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä—É–ø–ø, –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π ANOVA)
group1 = np.random.exponential(2, 30)
group2 = np.random.exponential(2.3, 30)
group3 = np.random.exponential(2.5, 30)

h_stat, p_value = kruskal(group1, group2, group3)
print(f"\nKruskal-Wallis —Ç–µ—Å—Ç:")
print(f"H = {h_stat:.4f}, p-value = {p_value:.4f}")

# BOOTSTRAP (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥)
def bootstrap_mean_diff(sample1, sample2, n_bootstrap=10000):
    diffs = []
    for _ in range(n_bootstrap):
        sample1_boot = np.random.choice(sample1, len(sample1), replace=True)
        sample2_boot = np.random.choice(sample2, len(sample2), replace=True)
        diffs.append(np.mean(sample1_boot) - np.mean(sample2_boot))
    
    ci_lower = np.percentile(diffs, 2.5)
    ci_upper = np.percentile(diffs, 97.5)
    
    return ci_lower, ci_upper

ci_lower, ci_upper = bootstrap_mean_diff(sample1, sample2)
print(f"Bootstrap 95% CI –¥–ª—è —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥–Ω–∏—Ö: [{ci_lower:.4f}, {ci_upper:.4f}]")

if ci_lower < 0 < ci_upper:
    print("‚úó CI —Å–æ–¥–µ—Ä–∂–∏—Ç 0 ‚Üí —Ä–∞–∑–ª–∏—á–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã")
else:
    print("‚úì CI –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 0 ‚Üí —Ä–∞–∑–ª–∏—á–∏—è –∑–Ω–∞—á–∏–º—ã")
```

### –°–≤—è–∑–Ω—ã–µ –∏ –Ω–µ—Å–≤—è–∑–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏

```python
# –ù–ï–°–í–Ø–ó–ù–´–ï (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ) –≤—ã–±–æ—Ä–∫–∏:
# - –†–∞–∑–Ω—ã–µ –ª—é–¥–∏ –≤ –≥—Ä—É–ø–ø–∞—Ö
# - –ù–µ—Ç –ø–∞–∏—Ä–æ–≤–∞–Ω–∏—è
# –¢–µ—Å—Ç—ã: t-—Ç–µ—Å—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö, Mann-Whitney U

# –°–í–Ø–ó–ù–´–ï (–∑–∞–≤–∏—Å–∏–º—ã–µ) –≤—ã–±–æ—Ä–∫–∏:
# - –û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –æ–±—ä–µ–∫—Ç –∏–∑–º–µ—Ä–µ–Ω –¥–≤–∞–∂–¥—ã (–¥–æ/–ø–æ—Å–ª–µ)
# - –ü–æ–ø–∞—Ä–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
# –¢–µ—Å—Ç—ã: –ø–∞—Ä–Ω—ã–π t-—Ç–µ—Å—Ç, Wilcoxon signed-rank

# –ü—Ä–∏–º–µ—Ä –ø–∞—Ä–Ω–æ–≥–æ t-—Ç–µ—Å—Ç–∞
from scipy.stats import ttest_rel

before_treatment = np.array([100, 102, 98, 101, 99])
after_treatment = np.array([98, 100, 95, 99, 97])

t_stat, p_value = ttest_rel(before_treatment, after_treatment)
print(f"–ü–∞—Ä–Ω—ã–π t-—Ç–µ—Å—Ç: t={t_stat:.4f}, p-value={p_value:.4f}")
```

### A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
# A/B —Ç–µ—Å—Ç - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (A vs B)

# –í–∞—Ä–∏–∞–Ω—Ç A: –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å–∞–π—Ç
clicks_a = 45
impressions_a = 1000
ctr_a = clicks_a / impressions_a

# –í–∞—Ä–∏–∞–Ω—Ç B: –Ω–æ–≤—ã–π —Å–∞–π—Ç
clicks_b = 60
impressions_b = 1000
ctr_b = clicks_b / impressions_b

# –î–≤—É—Ö–≤—ã–±–æ—Ä–æ—á–Ω—ã–π z-—Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
from statsmodels.stats.proportion import proportions_ztest

count = np.array([clicks_a, clicks_b])
nobs = np.array([impressions_a, impressions_b])

z_stat, p_value = proportions_ztest(count, nobs)
print(f"A/B —Ç–µ—Å—Ç:")
print(f"CTR A: {ctr_a:.4f}, CTR B: {ctr_b:.4f}")
print(f"z-statistic: {z_stat:.4f}, p-value: {p_value:.4f}")

if p_value < 0.05:
    print("‚úì –í–∞—Ä–∏–∞–Ω—Ç B –∑–Ω–∞—á–∏–º–æ –ª—É—á—à–µ")
else:
    print("‚úó –†–∞–∑–ª–∏—á–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã")
```

### –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑

```python
# –ü—Ä–æ–±–ª–µ–º–∞: –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å 20 –≥–∏–ø–æ—Ç–µ–∑ —Å Œ±=0.05,
# –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —Ä–æ–¥–∞ —Ä–∞—Å—Ç–µ—Ç!

# –†–µ—à–µ–Ω–∏–µ 1: Bonferroni –∫–æ—Ä—Ä–µ–∫—Ü–∏—è
n_tests = 20
alpha_bonferroni = 0.05 / n_tests  # 0.0025
print(f"Bonferroni alpha: {alpha_bonferroni:.4f}")

# –†–µ—à–µ–Ω–∏–µ 2: FDR (False Discovery Rate)
from scipy.stats import norm

p_values = np.array([0.001, 0.01, 0.05, 0.1, 0.2, 0.5])
m = len(p_values)
rank = np.arange(1, m + 1)
fdr_threshold = (rank / m) * 0.05

# –û—Ç–∫–ª–æ–Ω—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã –≥–¥–µ p_value < fdr_threshold[rank-1]
print(f"FDR –ø–æ—Ä–æ–≥–∏: {fdr_threshold}")
```

---

## –¢–ï–ú–ê 6: –†–ï–ì–†–ï–°–°–ò–Ø

### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

```python
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
X = np.random.randn(100, 1)
y = 2.5 * X.ravel() + np.random.randn(100) + 1

# SKLEARN
model = LinearRegression()
model.fit(X, y)

print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (–Ω–∞–∫–ª–æ–Ω): {model.coef_[0]:.4f}")
print(f"Intercept (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ): {model.intercept_:.4f}")
print(f"R¬≤ Score: {model.score(X, y):.4f}")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = model.predict(X)

# –û—Å—Ç–∞—Ç–∫–∏
residuals = y - y_pred

# MAE, MSE, RMSE
mae = np.mean(np.abs(residuals))
mse = np.mean(residuals**2)
rmse = np.sqrt(mse)

print(f"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}")
```

### –†–µ–≥—Ä–µ—Å—Å–∏—è —Å TensorFlow

```python
import tensorflow as tf
import numpy as np

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
X = np.random.randn(100, 1).astype(np.float32)
y = 2.5 * X.ravel() + np.random.randn(100).astype(np.float32) + 1

# –ú–æ–¥–µ–ª—å
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)  # Linear output
])

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=50, verbose=0)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = model.predict(X).flatten()
r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)
print(f"R¬≤ TensorFlow: {r2:.4f}")
```

### –ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—è (AR)

```python
from statsmodels.tsa.ar_model import AutoReg
import numpy as np
import pandas as pd

# –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
np.random.seed(42)
data = np.cumsum(np.random.randn(100))

# AutoReg –º–æ–¥–µ–ª—å
model = AutoReg(data, lags=5)  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 5 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
fitted_model = model.fit()

print(fitted_model.summary())

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
predictions = fitted_model.predict(start=5, end=99)
```

---

## –¢–ï–ú–ê 7: –ú–ï–¢–û–î–´ –û–¶–ï–ù–ö–ò –ó–ù–ê–ß–ò–ú–û–°–¢–ò –ü–ê–†–ê–ú–ï–¢–†–û–í

```python
from scipy import stats
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

# –ú–ï–¢–û–î 1: T-–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò P-VALUE

X = np.random.randn(100, 1)
y = 2.5 * X.ravel() + np.random.randn(100) + 1

# –î–æ–±–∞–≤–ª—è–µ–º intercept
X_with_const = np.column_stack([np.ones(len(X)), X])

# –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –ú–ù–ö (Least Squares)
beta = np.linalg.inv(X_with_const.T @ X_with_const) @ X_with_const.T @ y

# –û—Å—Ç–∞—Ç–∫–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
residuals = y - X_with_const @ beta
s_squared = np.sum(residuals**2) / (len(y) - X_with_const.shape[1])

# –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏
var_beta = s_squared * np.linalg.inv(X_with_const.T @ X_with_const)
se_beta = np.sqrt(np.diag(var_beta))

# T-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
t_stats = beta / se_beta

# P-values (–¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Ç–µ—Å—Ç)
p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), len(y) - 2))

print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã: {beta}")
print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏: {se_beta}")
print(f"T-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {t_stats}")
print(f"P-values: {p_values}")

# –ú–ï–¢–û–î 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º statsmodels
import statsmodels.api as sm

X_sm = sm.add_constant(X)
model = sm.OLS(y, X_sm)
results = model.fit()
print(results.summary())

# –ú–ï–¢–û–î 3: Bootstrap –¥–ª—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
def bootstrap_ci(X, y, n_bootstrap=1000):
    n = len(X)
    coefficients = []
    
    for _ in range(n_bootstrap):
        indices = np.random.choice(n, n, replace=True)
        X_boot = X[indices]
        y_boot = y[indices]
        
        X_boot_const = np.column_stack([np.ones(n), X_boot])
        beta_boot = np.linalg.inv(X_boot_const.T @ X_boot_const) @ X_boot_const.T @ y_boot
        coefficients.append(beta_boot[1])  # —Ç–æ–ª—å–∫–æ slope
    
    coefficients = np.array(coefficients)
    ci_lower = np.percentile(coefficients, 2.5)
    ci_upper = np.percentile(coefficients, 97.5)
    
    return ci_lower, ci_upper

ci_l, ci_u = bootstrap_ci(X, y)
print(f"\nBootstrap 95% CI –¥–ª—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞: [{ci_l:.4f}, {ci_u:.4f}]")
```

---

## –¢–ï–ú–ê 8: –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø

### KMeans

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
import numpy as np
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
X = np.random.randn(300, 2)
X[:100] += np.array([5, 5])
X[100:200] += np.array([10, 0])

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

print(f"–¶–µ–Ω—Ç—Ä—ã: {kmeans.cluster_centers_}")
print(f"–ò–Ω–µ—Ä—Ü–∏—è (—Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏): {kmeans.inertia_:.4f}")
```

### DBSCAN

```python
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
print(f"–®—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise}")
```

### –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è

```python
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist

# –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º –£–æ—Ä–¥–∞ (Ward)
Z = linkage(X, method='ward')

# –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞
dendrogram(Z)

# –ü–æ–ª—É—á–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –¥–∏—Å—Ç–∞–Ω—Ü–∏–µ–π 5
clusters = fcluster(Z, t=5, criterion='distance')
print(f"–ö–ª–∞—Å—Ç–µ—Ä—ã: {np.unique(clusters)}")
```

### –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

```python
# –í–ù–£–¢–†–ï–ù–ù–ò–ï –ú–ï–¢–†–ò–ö–ò (–±–µ–∑ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫)

# Silhouette Score (-1 –¥–æ 1, –≤—ã—à–µ –ª—É—á—à–µ)
silhouette = silhouette_score(X, labels)
print(f"Silhouette Score: {silhouette:.4f}")

# Davies-Bouldin Index (–Ω–∏–∂–µ –ª—É—á—à–µ)
db_index = davies_bouldin_score(X, labels)
print(f"Davies-Bouldin Index: {db_index:.4f}")

# Calinski-Harabasz Index (–≤—ã—à–µ –ª—É—á—à–µ)
from sklearn.metrics import calinski_harabasz_score
ch_index = calinski_harabasz_score(X, labels)
print(f"Calinski-Harabasz Index: {ch_index:.4f}")

# –í–ù–ï–®–ù–ò–ï –ú–ï–¢–†–ò–ö–ò (—Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏)
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

true_labels = np.array([0]*100 + [1]*100 + [2]*100)

ari = adjusted_rand_score(true_labels, labels)
nmi = normalized_mutual_info_score(true_labels, labels)

print(f"Adjusted Rand Index: {ari:.4f}")
print(f"Normalized Mutual Info: {nmi:.4f}")
```

### –í—ã–±–æ—Ä —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

```python
# –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è (Elbow Method)
inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X)
    inertias.append(km.inertia_)
    silhouettes.append(silhouette_score(X, km.labels_))

# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π k - –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è "–ª–æ–∫–æ—Ç—å"
plt.plot(K_range, inertias, 'o-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.show()
```

---

## –¢–ï–ú–ê 9: –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï (Topic Modeling)

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

# –î–æ–∫—É–º–µ–Ω—Ç—ã
documents = [
    "Python machine learning deep learning",
    "data science statistics analysis",
    "neural networks AI artificial intelligence",
    "clustering classification supervised learning",
    "regression prediction model"
]

# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Ç–µ—Ä–º–∏–Ω–æ–≤-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
vectorizer = CountVectorizer(max_features=20, stop_words='english')
doc_term_matrix = vectorizer.fit_transform(documents)

# LDA –º–æ–¥–µ–ª—å (Latent Dirichlet Allocation)
lda = LatentDirichletAllocation(
    n_components=2,  # 2 —Ç–µ–º—ã
    random_state=42,
    max_iter=20
)

lda.fit(doc_term_matrix)

# –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–π —Ç–µ–º–µ
feature_names = vectorizer.get_feature_names_out()

for topic_idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[-5:][::-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"–¢–µ–º–∞ {topic_idx}: {', '.join(top_words)}")

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
doc_topic_dist = lda.transform(doc_term_matrix)
print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –≤ –ø–µ—Ä–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ: {doc_topic_dist[0]}")
```

---

## –¢–ï–ú–ê 10: –û–†–¢–û–ì–û–ù–ê–õ–¨–ù–´–ï –ú–ê–¢–†–ò–ß–ù–´–ï –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø

### –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –•–∞–∞—Ä–∞

```python
import numpy as np
from scipy.fftpack import dct

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –•–∞–∞—Ä–∞ (–ø—Ä–æ—Å—Ç–æ–µ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ)
def haar_transform(signal):
    """–û–¥–Ω–æ–º–µ—Ä–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –•–∞–∞—Ä–∞"""
    n = len(signal)
    if n == 1:
        return signal
    
    # Divide and average
    averages = (signal[::2] + signal[1::2]) / np.sqrt(2)
    differences = (signal[::2] - signal[1::2]) / np.sqrt(2)
    
    return np.concatenate([averages, differences])

def inverse_haar_transform(transformed):
    """–û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –•–∞–∞—Ä–∞"""
    n = len(transformed)
    half = n // 2
    
    averages = transformed[:half]
    differences = transformed[half:]
    
    signal = np.zeros(n)
    signal[::2] = (averages + differences) / np.sqrt(2)
    signal[1::2] = (averages - differences) / np.sqrt(2)
    
    return signal

# –ü—Ä–∏–º–µ—Ä
signal = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=float)
transformed = haar_transform(signal)
reconstructed = inverse_haar_transform(transformed)

print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Å–∏–≥–Ω–∞–ª: {signal}")
print(f"–ü–æ—Å–ª–µ –•–∞–∞—Ä–∞: {transformed}")
print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π: {reconstructed}")
```

### –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –£–æ–ª—à–∞

```python
# –ú–∞—Ç—Ä–∏—Ü–∞ –ê–¥–∞–º–∞—Ä–∞ (–æ—Å–Ω–æ–≤–∞ –£–æ–ª—à–∞)
def hadamard_matrix(n):
    """–°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ê–¥–∞–º–∞—Ä–∞ —Ä–∞–∑–º–µ—Ä–æ–º n x n (n = 2^k)"""
    if n == 1:
        return np.array([[1]])
    
    H = hadamard_matrix(n // 2)
    return np.vstack([
        np.hstack([H, H]),
        np.hstack([H, -H])
    ]) / np.sqrt(2)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –£–æ–ª—à–∞
def walsh_transform(signal):
    n = len(signal)
    W = hadamard_matrix(n)
    return W @ signal

# –°–ø–µ–∫—Ç—Ä –£–æ–ª—à–∞ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)
signal = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=float)
walsh_coeff = walsh_transform(signal)

print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –£–æ–ª—à–∞: {walsh_coeff}")
```

---

## –¢–ï–ú–ê 11: –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –§–£–†–¨–ï

```python
import numpy as np
from scipy.fft import fft, ifft, fftfreq
import matplotlib.pyplot as plt

# –°–∏–≥–Ω–∞–ª: —Å—É–º–º–∞ 3 —Å–∏–Ω—É—Å–æ–∏–¥
t = np.linspace(0, 1, 256, endpoint=False)
signal = np.sin(2*np.pi*5*t) + np.sin(2*np.pi*10*t) + 0.5*np.sin(2*np.pi*15*t)

# –ü—Ä—è–º–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –§—É—Ä—å–µ
fft_coeffs = fft(signal)
freqs = fftfreq(len(signal), t[1]-t[0])

# –°–ø–µ–∫—Ç—Ä –º–æ—â–Ω–æ—Å—Ç–∏ (–∞–º–ø–ª–∏—Ç—É–¥—ã)
power = np.abs(fft_coeffs) ** 2

# –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã
positive_freqs = freqs[:len(freqs)//2]
positive_power = power[:len(power)//2]

# –û—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã
top_freqs = positive_freqs[np.argsort(positive_power)[-3:]]
print(f"–û—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã: {np.sort(top_freqs)}")

# –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
reconstructed = np.real(ifft(fft_coeffs))
print(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {np.max(np.abs(signal - reconstructed)):.10f}")

# –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ (Fourier Spectrogram)
from scipy.signal import spectrogram
f, t_spec, Sxx = spectrogram(signal, nperseg=64)
print(f"–ß–∞—Å—Ç–æ—Ç—ã —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã: {f}")
print(f"–í—Ä–µ–º—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã: {t_spec}")
```

---

## –¢–ï–ú–ê 12: –û–ö–û–ù–ù–û–ï –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –§–£–†–¨–ï –ò –í–†–ï–ú–ï–ù–ù–û-–ß–ê–°–¢–û–¢–ù–´–ô –ê–ù–ê–õ–ò–ó

```python
from scipy.signal import stft, istft
from scipy.signal.windows import hann
import numpy as np
import matplotlib.pyplot as plt

# –°–∏–≥–Ω–∞–ª —Å –∏–∑–º–µ–Ω—è—é—â–∏–º–∏—Å—è —á–∞—Å—Ç–æ—Ç–∞–º–∏
t = np.linspace(0, 2, 512)
signal = np.sin(2*np.pi*5*t) * (t < 1) + np.sin(2*np.pi*20*t) * (t >= 1)

# –û–∫–æ–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –§—É—Ä—å–µ (Short-Time Fourier Transform)
f, t_stft, Zxx = stft(signal, fs=256, window='hann', nperseg=64)

print(f"STFT shape: {Zxx.shape}")  # (frequency, time)

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–∏–≥–Ω–∞–ª–∞
t_recon, signal_recon = istft(Zxx, fs=256, window='hann', nperseg=64)
print(f"–û—à–∏–±–∫–∞: {np.max(np.abs(signal - signal_recon[:len(signal)])):.10f}")

# ARIMA –º–æ–¥–µ–ª—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
from statsmodels.tsa.arima.model import ARIMA

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
np.random.seed(42)
ts = np.cumsum(np.random.randn(100))

# ARIMA(1,1,1) - AutoRegressive Integrated Moving Average
model = ARIMA(ts, order=(1, 1, 1))
fitted_model = model.fit()

print(fitted_model.summary())

# –ü—Ä–æ–≥–Ω–æ–∑
forecast = fitted_model.get_forecast(steps=10)
print(f"–ü—Ä–æ–≥–Ω–æ–∑: {forecast.predicted_mean.values}")

# SARIMAX (—Å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å—é)
from statsmodels.tsa.statespace.sarimax import SARIMAX

# SARIMAX(1,1,1)x(1,1,1,12) - —Å —Å–µ–∑–æ–Ω–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º (–ø–µ—Ä–∏–æ–¥=12)
model_seasonal = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))
fitted_seasonal = model_seasonal.fit()
```

---

## –¢–ï–ú–ê 13: –í–´–ë–†–û–°–´ –ò –ê–í–¢–û–≠–ù–ö–û–î–ï–†–´

### –ú–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤

```python
import numpy as np
from scipy import stats
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest

# –î–∞–Ω–Ω—ã–µ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏
X = np.random.randn(100, 2)
X = np.vstack([X, np.array([[10, 10], [10, -10], [-10, 10]])])

# –ú–ï–¢–û–î 1: Z-Score
z_scores = np.abs(stats.zscore(X))
outliers_zscore = (z_scores > 3).any(axis=1)
print(f"Z-Score –≤—ã–±—Ä–æ—Å—ã: {np.sum(outliers_zscore)}")

# –ú–ï–¢–û–î 2: IQR (Interquartile Range)
Q1 = np.percentile(X, 25, axis=0)
Q3 = np.percentile(X, 75, axis=0)
IQR = Q3 - Q1
outliers_iqr = ((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)
print(f"IQR –≤—ã–±—Ä–æ—Å—ã: {np.sum(outliers_iqr)}")

# –ú–ï–¢–û–î 3: Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
outliers_if = iso_forest.fit_predict(X) == -1
print(f"Isolation Forest –≤—ã–±—Ä–æ—Å—ã: {np.sum(outliers_if)}")

# –ú–ï–¢–û–î 4: Elliptic Envelope (Mahalanobis distance)
elliptic = EllipticEnvelope(contamination=0.05, random_state=42)
outliers_elliptic = elliptic.fit_predict(X) == -1
print(f"Elliptic Envelope –≤—ã–±—Ä–æ—Å—ã: {np.sum(outliers_elliptic)}")
```

### –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã

```python
import tensorflow as tf
import numpy as np

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
normal_data = np.random.randn(1000, 10)
anomaly_data = np.random.uniform(-5, 5, (50, 10))  # –í—ã–±—Ä–æ—Å—ã
X_train = normal_data[:900]
X_val_normal = normal_data[900:]
X_val_anomaly = anomaly_data

# –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
autoencoder = tf.keras.Sequential([
    tf.keras.layers.Dense(8, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(4, activation='relu'),  # Bottleneck
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(10, activation='linear')
])

autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.fit(X_train, X_train, epochs=50, verbose=0)

# –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (reconstruction error)
train_predictions = autoencoder.predict(X_train)
train_mse = np.mean(np.square(X_train - train_predictions), axis=1)
threshold = np.percentile(train_mse, 95)  # 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
val_normal_pred = autoencoder.predict(X_val_normal)
val_anomaly_pred = autoencoder.predict(X_val_anomaly)

mse_normal = np.mean(np.square(X_val_normal - val_normal_pred), axis=1)
mse_anomaly = np.mean(np.square(X_val_anomaly - val_anomaly_pred), axis=1)

print(f"–ü–æ—Ä–æ–≥: {threshold:.4f}")
print(f"–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö: {np.mean(mse_normal):.4f}")
print(f"–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π: {np.mean(mse_anomaly):.4f}")

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
anomaly_detected_normal = np.sum(mse_normal > threshold)
anomaly_detected_anomaly = np.sum(mse_anomaly > threshold)
print(f"–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è: {anomaly_detected_normal}/{len(mse_normal)}")
print(f"–í–µ—Ä–Ω—ã–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {anomaly_detected_anomaly}/{len(mse_anomaly)}")
```

---

## –¢–ï–ú–ê 14: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ï–ô (Feature Extraction)

### –ü–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (Dimensionality Reduction)

```python
import numpy as np
from sklearn.decomposition import PCA, TSNE
from sklearn.manifold import UMAP

# –í—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
X = np.random.randn(1000, 100)  # 1000 –æ–±—ä–µ–∫—Ç–æ–≤, 100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

# –ú–ï–¢–û–î 1: PCA (–ª–∏–Ω–µ–π–Ω—ã–π –º–µ—Ç–æ–¥)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è: {pca.explained_variance_ratio_}")
print(f"–°—É–º–º–∞: {np.sum(pca.explained_variance_ratio_):.4f}")

# –í—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
pca_full = PCA()
pca_full.fit(X)

cumsum = np.cumsum(pca_full.explained_variance_ratio_)
n_components = np.argmax(cumsum >= 0.95) + 1  # 95% –≤–∞—Ä–∏–∞—Ü–∏–∏
print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è 95% –≤–∞—Ä–∏–∞—Ü–∏–∏: {n_components}")

# –ú–ï–¢–û–î 2: t-SNE (–Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π –º–µ—Ç–æ–¥)
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# –ú–ï–¢–û–î 3: UMAP (–±—ã—Å—Ç—Ä–µ–µ t-SNE)
umap_reducer = UMAP(n_components=2)
X_umap = umap_reducer.fit_transform(X)
```

### –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (Feature Generation)

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
X = np.random.randn(100, 2)

# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

print(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {X.shape}")
print(f"–ü–æ—Å–ª–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_poly.shape}")
# –°–æ–¥–µ—Ä–∂–∏—Ç: [X1, X2, X1^2, X1*X2, X2^2]

# –î—Ä—É–≥–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
# Log, sqrt, exp —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
X_log = np.log1p(np.abs(X))
X_sqrt = np.sqrt(np.abs(X))

# –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
from sklearn.preprocessing import PolynomialFeatures
interaction_terms = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_interaction = interaction_terms.fit_transform(X)
```

---

## –¢–ï–ú–ê 15: –û–¢–¢–û–ö (Churn Prediction)

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix

# –°–∏–º—É–ª–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ–± –æ—Ç—á–∏—Å–ª–µ–Ω–∏–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
np.random.seed(42)
n_students = 1000

data = pd.DataFrame({
    'gpa': np.random.normal(3.2, 0.6, n_students),
    'attendance': np.random.uniform(0.5, 1, n_students),
    'assignment_completion': np.random.uniform(0, 1, n_students),
    'library_visits': np.random.poisson(10, n_students),
    'office_hours': np.random.poisson(5, n_students),
    'family_income': np.random.exponential(50000, n_students)
})

# –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –æ—Ç—Ç–æ–∫ (1 = –æ—Ç—á–∏—Å–ª–∏–ª—Å—è/–æ—Ç–∫–∞–∑–∞–ª—Å—è, 0 = –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç)
# –ß–µ–º –≤—ã—à–µ GPA –∏ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å, —Ç–µ–º –Ω–∏–∂–µ —Ä–∏—Å–∫ –æ—Ç—á–∏—Å–ª–µ–Ω–∏—è
churn_prob = 0.9 - (data['gpa'] / 5) * 0.3 - (data['attendance'] * 0.2)
churn_prob = np.clip(churn_prob, 0, 1)
data['churn'] = np.random.binomial(1, churn_prob)

print(f"–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞: {data['churn'].mean():.2%}")

# TRAIN-TEST SPLIT
X = data.drop('churn', axis=1)
y = data['churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ú–û–î–ï–õ–¨: Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
y_pred = model.predict(X_test_scaled)

# –û–¶–ï–ù–ö–ò
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC-AUC: {roc_auc:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
sensitivity = tp / (tp + fn)  # True Positive Rate
specificity = tn / (tn + fp)  # True Negative Rate
print(f"Sensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# –ö—Ä–∏–≤–∞—è Precision-Recall
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)

# Feature Importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n{feature_importance}")

# –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø
print("\n–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤:")
print("1. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å GPA - –≥–ª–∞–≤–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä –æ—Ç—Ç–æ–∫–∞")
print("2. –£–ª—É—á—à–∏—Ç—å –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –ª–µ–∫—Ü–∏—è—Ö")
print("3. –û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –Ω–∏–∑–∫–∏–º GPA (<2.5)")
print("4. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –Ω–∞ –±–∞–∑–µ –º–æ–¥–µ–ª–∏")
```

---

## ‚ö° –ë–´–°–¢–†–ê–Ø –°–ü–†–ê–í–ö–ê –ù–ê –≠–ö–ó–ê–ú–ï–ù–ï

### –ß—Ç–æ –≤—ã–≤–æ–¥–∏—Ç—å –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ

```
–ö–û–î:
1. –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–∞–∂–¥–∞—è —Ñ—É–Ω–∫—Ü–∏—è?
2. –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
3. –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
4. –§–∏–∑–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª

–û–®–ò–ë–ö–ê:
- –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ 100-150
- –ù–∞–π—Ç–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –æ—à–∏–±–∫—É –ò–õ–ò –ª–æ–≥–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É
- –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

–ú–ï–¢–û–î:
- –ö–∞–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç?
- –ö–∞–∫–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º?
- –ö–∞–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è?

–í–´–í–û–î:
- –ù—É–∂–Ω–∞ –ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è?
- –ï—Å—Ç—å –ª–∏ –≤—ã–±—Ä–æ—Å—ã?
- –ß—Ç–æ –±—É–¥–µ—Ç –Ω–∞ –ø–µ—á–∞—Ç–∏?
```

### –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–µ–º—ã

```python
# –í—Å–µ–≥–¥–∞ –Ω—É–∂–Ω—ã:
import numpy as np
import pandas as pd
from scipy import stats
from sklearn import preprocessing, model_selection, metrics
import matplotlib.pyplot as plt

# –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã
stats.ttest_ind(a, b)  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 2 –≥—Ä—É–ø–ø
stats.f_oneway(*groups)  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ 3+ –≥—Ä—É–ø–ø
stats.chi2_contingency(table)  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
stats.pearsonr(x, y)  # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è

# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
```

### –¢–∏–ø–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã

1. **"–ö–∞–∫–æ–π –º–µ—Ç–æ–¥?"** ‚Üí –°–º–æ—Ç—Ä–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—á–∏—Å–ª–æ–≤—ã–µ/–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ)
2. **"–û—à–∏–±–∫–∞?"** ‚Üí –°–∏–Ω—Ç–∞–∫—Å–∏—Å + –ª–æ–≥–∏–∫–∞ (–∏–Ω–¥–µ–∫—Å—ã, —Ç–∏–ø—ã, —Ä–∞–∑–º–µ—Ä—ã)
3. **"–í—ã–≤–æ–¥?"** ‚Üí –¢—Ä–∞—Å—Å–∏—Ä—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤–æ
4. **"–ö–æ–Ω—Å–ø–µ–∫—Ç?"** ‚Üí –ù–∞–∑–æ–≤–∏: –ß—Ç–æ? –ó–∞—á–µ–º? –ö–∞–∫? –ö–æ–≥–¥–∞?
